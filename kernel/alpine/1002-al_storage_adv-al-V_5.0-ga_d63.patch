diff --git a/arch/arm/include/asm/elf.h b/arch/arm/include/asm/elf.h
index 56211f2..9a8d739 100644
--- a/arch/arm/include/asm/elf.h
+++ b/arch/arm/include/asm/elf.h
@@ -109,7 +109,7 @@ int dump_task_regs(struct task_struct *t, elf_gregset_t *elfregs);
 #define ELF_CORE_COPY_TASK_REGS dump_task_regs
 
 #define CORE_DUMP_USE_REGSET
-#define ELF_EXEC_PAGESIZE	4096
+#define ELF_EXEC_PAGESIZE	PAGE_SIZE
 
 /* This is the location that an ET_DYN program is loaded if exec'ed.  Typical
    use of this is to invoke "./ld.so someprog" to test out a new version of
diff --git a/arch/arm/include/asm/fixmap.h b/arch/arm/include/asm/fixmap.h
index bbae919..d2b5985 100644
--- a/arch/arm/include/asm/fixmap.h
+++ b/arch/arm/include/asm/fixmap.h
@@ -13,8 +13,13 @@
  * 0xfffe0000 and 0xfffeffff.
  */
 
+#if defined(CONFIG_ARM_PAGE_SIZE_LARGE) && defined(CONFIG_HIGHMEM)
+#define FIXADDR_START		0xff400000UL
+#define FIXADDR_TOP		0xff800000UL
+#else
 #define FIXADDR_START		0xfff00000UL
 #define FIXADDR_TOP		0xfffe0000UL
+#endif
 #define FIXADDR_SIZE		(FIXADDR_TOP - FIXADDR_START)
 
 #define FIX_KMAP_BEGIN		0
diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 57870ab..76d7bc5 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -80,7 +80,17 @@
  */
 #define IOREMAP_MAX_ORDER	24
 
+
+/*
+ * Size of DMA-consistent memory region.  Must be multiple of 2M,
+ * between 2MB and 14MB inclusive.
+ */
+#ifndef CONSISTENT_DMA_SIZE
+#define CONSISTENT_DMA_SIZE 	SZ_2M
+#endif
+
 #define CONSISTENT_END		(0xffe00000UL)
+#define CONSISTENT_BASE		(CONSISTENT_END - CONSISTENT_DMA_SIZE)
 
 #else /* CONFIG_MMU */
 
diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index cbdc7a2..85b9604 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -11,10 +11,22 @@
 #define _ASMARM_PAGE_H
 
 /* PAGE_SHIFT determines the page size */
+#ifdef CONFIG_ARM_PAGE_SIZE_LARGE
+#define PAGE_SHIFT		CONFIG_ARM_PAGE_SIZE_LARGE_SHIFT
+#else
 #define PAGE_SHIFT		12
+#endif
 #define PAGE_SIZE		(_AC(1,UL) << PAGE_SHIFT)
 #define PAGE_MASK		(~(PAGE_SIZE-1))
 
+/* H/W pages are always 4KB.
+ * A single linux page may be implemented using more than one H/W page.
+ */
+#define HW_PAGE_SHIFT		12
+#define HW_PAGE_SIZE		(1 << HW_PAGE_SHIFT)
+#define HW_PAGE_MASK		(~(HW_PAGE_SIZE-1))
+#define HW_PAGES_PER_PAGE	(1 << (PAGE_SHIFT - HW_PAGE_SHIFT))
+
 #ifndef __ASSEMBLY__
 
 #ifndef CONFIG_MMU
diff --git a/arch/arm/include/asm/pgtable-2level.h b/arch/arm/include/asm/pgtable-2level.h
index f97ee02..aa0fac1 100644
--- a/arch/arm/include/asm/pgtable-2level.h
+++ b/arch/arm/include/asm/pgtable-2level.h
@@ -74,7 +74,8 @@
 
 #define PTE_HWTABLE_PTRS	(PTRS_PER_PTE)
 #define PTE_HWTABLE_OFF		(PTE_HWTABLE_PTRS * sizeof(pte_t))
-#define PTE_HWTABLE_SIZE	(PTRS_PER_PTE * sizeof(u32))
+#define PTE_HWTABLE_SIZE	(PTE_HWTABLE_PTRS * sizeof(u32))
+#define PTE_HWTABLE_MASK	(~((PTE_HWTABLE_SIZE*2)-1))
 
 /*
  * PMD_SHIFT determines the size of the area a second-level page table can map
diff --git a/arch/arm/include/asm/pgtable-3level-types.h b/arch/arm/include/asm/pgtable-3level-types.h
index 921aa30..2f5aeb6 100644
--- a/arch/arm/include/asm/pgtable-3level-types.h
+++ b/arch/arm/include/asm/pgtable-3level-types.h
@@ -33,7 +33,11 @@ typedef u64 pgdval_t;
 /*
  * These are used to make use of C type-checking..
  */
-typedef struct { pteval_t pte; } pte_t;
+typedef struct { pteval_t pte;
+#if HW_PAGES_PER_PAGE > 1
+	pteval_t unused[HW_PAGES_PER_PAGE-1];
+#endif /* HW_PAGES_PER_PAGE > 1 */
+} pte_t;
 typedef struct { pmdval_t pmd; } pmd_t;
 typedef struct { pgdval_t pgd; } pgd_t;
 typedef struct { pteval_t pgprot; } pgprot_t;
@@ -43,24 +47,29 @@ typedef struct { pteval_t pgprot; } pgprot_t;
 #define pgd_val(x)	((x).pgd)
 #define pgprot_val(x)   ((x).pgprot)
 
-#define __pte(x)        ((pte_t) { (x) } )
+#define __pte(x)        ({pte_t __pte = { .pte = (x) }; __pte; })
 #define __pmd(x)        ((pmd_t) { (x) } )
 #define __pgd(x)	((pgd_t) { (x) } )
 #define __pgprot(x)     ((pgprot_t) { (x) } )
 
 #else	/* !STRICT_MM_TYPECHECKS */
 
-typedef pteval_t pte_t;
+typedef struct { pteval_t pte;
+#if HW_PAGES_PER_PAGE > 1
+	pteval_t unused[HW_PAGES_PER_PAGE-1];
+#endif /* HW_PAGES_PER_PAGE > 1 */
+} pte_t;
+
 typedef pmdval_t pmd_t;
 typedef pgdval_t pgd_t;
 typedef pteval_t pgprot_t;
 
-#define pte_val(x)	(x)
+#define pte_val(x)      ((x).pte)
 #define pmd_val(x)	(x)
 #define pgd_val(x)	(x)
 #define pgprot_val(x)	(x)
 
-#define __pte(x)	(x)
+#define __pte(x)        ({pte_t __pte = { .pte = (x) }; __pte; })
 #define __pmd(x)	(x)
 #define __pgd(x)	(x)
 #define __pgprot(x)	(x)
diff --git a/arch/arm/include/asm/pgtable-3level.h b/arch/arm/include/asm/pgtable-3level.h
index 54733e5..83da39e 100644
--- a/arch/arm/include/asm/pgtable-3level.h
+++ b/arch/arm/include/asm/pgtable-3level.h
@@ -29,13 +29,18 @@
  * There are enough spare bits in a page table entry for the kernel specific
  * state.
  */
+#ifdef CONFIG_ARM_PAGE_SIZE_LARGE
+#define PTRS_PER_PTE		(512 >> (CONFIG_ARM_PAGE_SIZE_LARGE_SHIFT - 12))
+#else
 #define PTRS_PER_PTE		512
+#endif
 #define PTRS_PER_PMD		512
 #define PTRS_PER_PGD		4
 
 #define PTE_HWTABLE_PTRS	(PTRS_PER_PTE)
 #define PTE_HWTABLE_OFF		(0)
-#define PTE_HWTABLE_SIZE	(PTRS_PER_PTE * sizeof(u64))
+#define PTE_HWTABLE_SIZE	(512 * 8) /*512 64bit values*/
+#define PTE_HWTABLE_MASK	(~(PTE_HWTABLE_SIZE-1))
 
 /*
  * PGDIR_SHIFT determines the size a top-level page table entry can map.
@@ -155,7 +160,7 @@
 
 static inline pmd_t *pud_page_vaddr(pud_t pud)
 {
-	return __va(pud_val(pud) & PHYS_MASK & (s32)PAGE_MASK);
+	return __va(pud_val(pud) & PHYS_MASK & (s32)PTE_HWTABLE_MASK);
 }
 
 /* Find an entry in the second-level page table.. */
diff --git a/arch/arm/include/asm/pgtable.h b/arch/arm/include/asm/pgtable.h
index eaedce7..64bcdd6 100644
--- a/arch/arm/include/asm/pgtable.h
+++ b/arch/arm/include/asm/pgtable.h
@@ -186,7 +186,7 @@ extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
 
 static inline pte_t *pmd_page_vaddr(pmd_t pmd)
 {
-	return __va(pmd_val(pmd) & PHYS_MASK & (s32)PAGE_MASK);
+	return __va(pmd_val(pmd) & PHYS_MASK & (s32)PTE_HWTABLE_MASK);
 }
 
 #define pmd_page(pmd)		pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
diff --git a/arch/arm/include/asm/shmparam.h b/arch/arm/include/asm/shmparam.h
index a5223b3..9530a6e 100644
--- a/arch/arm/include/asm/shmparam.h
+++ b/arch/arm/include/asm/shmparam.h
@@ -6,7 +6,11 @@
  * or page size, whichever is greater since the cache aliases
  * every size/ways bytes.
  */
+#ifdef CONFIG_ARM_PAGE_SIZE_LARGE
+#define	SHMLBA	(16 << 10)		 /* attach addr a multiple of this */
+#else
 #define	SHMLBA	(4 * PAGE_SIZE)		 /* attach addr a multiple of this */
+#endif
 
 /*
  * Enforce SHMLBA in shmat
diff --git a/arch/arm/include/asm/thread_info.h b/arch/arm/include/asm/thread_info.h
index f00b569..9f6ae09 100644
--- a/arch/arm/include/asm/thread_info.h
+++ b/arch/arm/include/asm/thread_info.h
@@ -14,9 +14,15 @@
 
 #include <linux/compiler.h>
 #include <asm/fpstate.h>
+#include <asm/page.h>
 
+#if (PAGE_SHIFT > 12)
+#define THREAD_SIZE_ORDER	0
+#else
 #define THREAD_SIZE_ORDER	1
-#define THREAD_SIZE		8192
+#endif
+
+#define THREAD_SIZE		(PAGE_SIZE << THREAD_SIZE_ORDER)
 #define THREAD_START_SP		(THREAD_SIZE - 8)
 
 #ifndef __ASSEMBLY__
diff --git a/arch/arm/include/asm/tlbflush.h b/arch/arm/include/asm/tlbflush.h
index c374592..641cf0d 100644
--- a/arch/arm/include/asm/tlbflush.h
+++ b/arch/arm/include/asm/tlbflush.h
@@ -374,12 +374,19 @@ local_flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 {
 	const int zero = 0;
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
+	unsigned long uaddr_last;
 
 	uaddr = (uaddr & PAGE_MASK) | ASID(vma->vm_mm);
+	uaddr_last = uaddr+PAGE_SIZE;
 
 	if (tlb_flag(TLB_WB))
 		dsb();
 
+	/*
+	 * normal case is HW_PAGE_SIZE==PAGE_SIZE,
+	 * After optimization, the for-loop will be gone
+	 */
+	for (; uaddr < uaddr_last; uaddr += HW_PAGE_SIZE) {
 	if (possible_tlb_flags & (TLB_V4_U_PAGE|TLB_V4_D_PAGE|TLB_V4_I_PAGE|TLB_V4_I_FULL) &&
 	    cpumask_test_cpu(smp_processor_id(), mm_cpumask(vma->vm_mm))) {
 		tlb_op(TLB_V4_U_PAGE, "c8, c7, 1", uaddr);
@@ -397,6 +404,7 @@ local_flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 #else
 	tlb_op(TLB_V7_UIS_PAGE, "c8, c3, 1", uaddr);
 #endif
+	}
 
 	if (tlb_flag(TLB_BARRIER))
 		dsb();
@@ -406,12 +414,15 @@ static inline void local_flush_tlb_kernel_page(unsigned long kaddr)
 {
 	const int zero = 0;
 	const unsigned int __tlb_flag = __cpu_tlb_flags;
+	unsigned long kaddr_last;
 
 	kaddr &= PAGE_MASK;
+	kaddr_last = kaddr+PAGE_SIZE;
 
 	if (tlb_flag(TLB_WB))
 		dsb();
 
+	for (; kaddr < kaddr_last; kaddr += HW_PAGE_SIZE) {
 	tlb_op(TLB_V4_U_PAGE, "c8, c7, 1", kaddr);
 	tlb_op(TLB_V4_D_PAGE, "c8, c6, 1", kaddr);
 	tlb_op(TLB_V4_I_PAGE, "c8, c5, 1", kaddr);
@@ -422,6 +433,7 @@ static inline void local_flush_tlb_kernel_page(unsigned long kaddr)
 	tlb_op(TLB_V6_D_PAGE, "c8, c6, 1", kaddr);
 	tlb_op(TLB_V6_I_PAGE, "c8, c5, 1", kaddr);
 	tlb_op(TLB_V7_UIS_PAGE, "c8, c3, 1", kaddr);
+	}
 
 	if (tlb_flag(TLB_BARRIER)) {
 		dsb();
diff --git a/arch/arm/kernel/entry-common.S b/arch/arm/kernel/entry-common.S
index 8c79344..dcf9a02 100644
--- a/arch/arm/kernel/entry-common.S
+++ b/arch/arm/kernel/entry-common.S
@@ -536,20 +536,11 @@ sys_fstatfs64_wrapper:
 		b	sys_fstatfs64
 ENDPROC(sys_fstatfs64_wrapper)
 
-/*
- * Note: off_4k (r5) is always units of 4K.  If we can't do the requested
- * offset, we return EINVAL.
- */
 sys_mmap2:
-#if PAGE_SHIFT > 12
-		tst	r5, #PGOFF_MASK
-		moveq	r5, r5, lsr #PAGE_SHIFT - 12
-		streq	r5, [sp, #4]
-		beq	sys_mmap_pgoff
-		mov	r0, #-EINVAL
-		mov	pc, lr
-#else
 		str	r5, [sp, #4]
+#if (PAGE_SHIFT > 12)
+		b	sys_arm_mmap_4koff
+#else
 		b	sys_mmap_pgoff
 #endif
 ENDPROC(sys_mmap2)
diff --git a/arch/arm/kernel/entry-header.S b/arch/arm/kernel/entry-header.S
index 160f337..0b53389 100644
--- a/arch/arm/kernel/entry-header.S
+++ b/arch/arm/kernel/entry-header.S
@@ -126,8 +126,8 @@
 	.endm
 
 	.macro	get_thread_info, rd
-	mov	\rd, sp, lsr #13
-	mov	\rd, \rd, lsl #13
+	mov	\rd, sp, lsr #(PAGE_SHIFT + THREAD_SIZE_ORDER)
+	mov	\rd, \rd, lsl #(PAGE_SHIFT + THREAD_SIZE_ORDER)
 	.endm
 
 	@
diff --git a/arch/arm/kernel/sys_arm.c b/arch/arm/kernel/sys_arm.c
index 3151f56..9e5b6d1 100644
--- a/arch/arm/kernel/sys_arm.c
+++ b/arch/arm/kernel/sys_arm.c
@@ -27,6 +27,7 @@
 #include <linux/ipc.h>
 #include <linux/uaccess.h>
 #include <linux/slab.h>
+#include <linux/printk.h>
 
 /*
  * Since loff_t is a 64 bit type we avoid a lot of ABI hassle
@@ -37,3 +38,26 @@ asmlinkage long sys_arm_fadvise64_64(int fd, int advice,
 {
 	return sys_fadvise64_64(fd, offset, len, advice);
 }
+
+#if (PAGE_SHIFT > 12)
+	/*
+	 * the "offeset" input variable is in always in 4k units for mmap2.
+	 * If PAGE_SIZE is different, we need shift it to present real pages,
+	 * and to make sure that the actual address is PAGE_SIZE aligned
+	 */
+asmlinkage unsigned long sys_arm_mmap_4koff(unsigned long addr,
+		unsigned long len, unsigned long prot, unsigned long flags,
+		unsigned long fd, unsigned long offset)
+{
+	unsigned long pgoff;
+	if (offset & ((PAGE_SIZE-1)>>12)) {
+		printk(KERN_WARNING
+				"mmap received unaligned request offset: %x.",
+				(unsigned int)offset);
+		return -EINVAL;
+	}
+	pgoff = offset >> (PAGE_SHIFT - 12);
+
+	return sys_mmap_pgoff(addr, len, prot, flags, fd, pgoff);
+}
+#endif
diff --git a/arch/arm/lib/copy_page.S b/arch/arm/lib/copy_page.S
index 6ee2f67..eb0f586 100644
--- a/arch/arm/lib/copy_page.S
+++ b/arch/arm/lib/copy_page.S
@@ -28,7 +28,7 @@ ENTRY(copy_page)
 		stmfd	sp!, {r4, lr}			@	2
 	PLD(	pld	[r1, #0]		)
 	PLD(	pld	[r1, #L1_CACHE_BYTES]		)
-		mov	r2, #COPY_COUNT			@	1
+		ldr	r2, =COPY_COUNT			@	1
 		ldmia	r1!, {r3, r4, ip, lr}		@	4+1
 1:	PLD(	pld	[r1, #2 * L1_CACHE_BYTES])
 	PLD(	pld	[r1, #3 * L1_CACHE_BYTES])
diff --git a/arch/arm/lib/uaccess.S b/arch/arm/lib/uaccess.S
index 5c908b1..f502272 100644
--- a/arch/arm/lib/uaccess.S
+++ b/arch/arm/lib/uaccess.S
@@ -15,10 +15,10 @@
 #include <asm/assembler.h>
 #include <asm/errno.h>
 #include <asm/domain.h>
+#include <asm/page.h>
 
 		.text
 
-#define PAGE_SHIFT 12
 
 /* Prototype: int __copy_to_user(void *to, const char *from, size_t n)
  * Purpose  : copy a block to user memory from kernel memory
diff --git a/arch/arm/mm/Kconfig b/arch/arm/mm/Kconfig
index 9ee7426..574c9c5 100644
--- a/arch/arm/mm/Kconfig
+++ b/arch/arm/mm/Kconfig
@@ -848,6 +848,66 @@ config CACHE_FEROCEON_L2_WRITETHROUGH
 	  Say Y here to use the Feroceon L2 cache in writethrough mode.
 	  Unless you specifically require this, say N for writeback mode.
 
+config ARM_PAGE_SIZE_LARGE
+	bool "Support large page sizes"
+	depends on CPU_V7 && ARM_LPAE && !HUGETLBFS && !TRANSPARENT_HUGEPAGE
+	default n
+	help
+	  Say Y to use page-sizes larger than 4KB.
+	  Advantages for large page size:
+		>Support for large storage volumes.
+		>Improvement for DMA (e.g. RAID) performance.
+	  Disadvantages for large pages:
+	    >Memory fragmentation increases.
+	    >Some existing software assumes 4KB page size, and may be incompatible.
+
+	  ARM architecture uses 4KB page. This options allows joining several H/W
+	  pages and using them as a single page in software.
+
+	  Unless you specifically require this, say N.
+
+choice
+	prompt "Page size to use"
+	default ARM_PAGE_SIZE_32KB
+	depends on ARM_PAGE_SIZE_LARGE && ARM_LPAE
+
+config ARM_PAGE_SIZE_64KB
+	bool "64KB pages"
+	help
+	  Sets page size to 64KB. Each linux-page is composed of 16 consecutive
+	  hardware pages. The consecutive hint bit is turned on, so all 16
+	  page-table entries are presented in H/W by a single TLB entry. This will
+	  minimise TLB misses.
+
+	  Binaries not specifically compiled for 64KB pages will probably crash.
+
+config ARM_PAGE_SIZE_32KB
+	bool "32KB pages"
+	help
+	  Sets page size to 32KB. Each linux-page is composed of 8 consecutive
+	  hardware pages.
+
+	  Binutills by default define maximum page size to be 32KB.
+	  Well-behaved, standard-compiled user-space programs will be able to
+	  work with a kernel using 32KB pages, even if not specifically
+	  compiled with this in mind.
+
+config ARM_PAGE_SIZE_4KB
+	bool "4KB pages"
+	help
+	  This setting will enable some large_page code, but keep the normal
+	  4KB pagesize.
+	  It is used for debugging.
+
+endchoice
+
+config ARM_PAGE_SIZE_LARGE_SHIFT
+	int
+	depends on ARM_PAGE_SIZE_LARGE
+	default 16 if ARM_PAGE_SIZE_64KB
+	default 15 if ARM_PAGE_SIZE_32KB
+	default 12 if ARM_PAGE_SIZE_4KB
+
 config MIGHT_HAVE_CACHE_L2X0
 	bool
 	help
diff --git a/arch/arm/mm/cache-v7.S b/arch/arm/mm/cache-v7.S
index 515b000..8db0bff 100644
--- a/arch/arm/mm/cache-v7.S
+++ b/arch/arm/mm/cache-v7.S
@@ -15,6 +15,7 @@
 #include <asm/assembler.h>
 #include <asm/errno.h>
 #include <asm/unwind.h>
+#include <asm/page.h>
 
 #include "proc-macros.S"
 
diff --git a/arch/arm/mm/highmem.c b/arch/arm/mm/highmem.c
index 21b9e1b..cae49f0 100644
--- a/arch/arm/mm/highmem.c
+++ b/arch/arm/mm/highmem.c
@@ -69,14 +69,14 @@ void *kmap_atomic(struct page *page)
 	 * With debugging enabled, kunmap_atomic forces that entry to 0.
 	 * Make sure it was indeed properly unmapped.
 	 */
-	BUG_ON(!pte_none(get_top_pte(vaddr)));
+	BUG_ON(!pte_none(get_fix_pte(vaddr)));
 #endif
 	/*
 	 * When debugging is off, kunmap_atomic leaves the previous mapping
 	 * in place, so the contained TLB flush ensures the TLB is updated
 	 * with the new mapping.
 	 */
-	set_top_pte(vaddr, mk_pte(page, kmap_prot));
+	set_fix_pte(vaddr, mk_pte(page, kmap_prot));
 
 	return (void *)vaddr;
 }
@@ -95,7 +95,7 @@ void __kunmap_atomic(void *kvaddr)
 			__cpuc_flush_dcache_area((void *)vaddr, PAGE_SIZE);
 #ifdef CONFIG_DEBUG_HIGHMEM
 		BUG_ON(vaddr != __fix_to_virt(FIX_KMAP_BEGIN + idx));
-		set_top_pte(vaddr, __pte(0));
+		set_fix_pte(vaddr, __pte(0));
 #else
 		(void) idx;  /* to kill a warning */
 #endif
@@ -119,9 +119,9 @@ void *kmap_atomic_pfn(unsigned long pfn)
 	idx = type + KM_TYPE_NR * smp_processor_id();
 	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
 #ifdef CONFIG_DEBUG_HIGHMEM
-	BUG_ON(!pte_none(get_top_pte(vaddr)));
+	BUG_ON(!pte_none(get_fix_pte(vaddr)));
 #endif
-	set_top_pte(vaddr, pfn_pte(pfn, kmap_prot));
+	set_fix_pte(vaddr, pfn_pte(pfn, kmap_prot));
 
 	return (void *)vaddr;
 }
@@ -133,5 +133,5 @@ struct page *kmap_atomic_to_page(const void *ptr)
 	if (vaddr < FIXADDR_START)
 		return virt_to_page(ptr);
 
-	return pte_page(get_top_pte(vaddr));
+	return pte_page(get_fix_pte(vaddr));
 }
diff --git a/arch/arm/mm/init.c b/arch/arm/mm/init.c
index 0ecc43f..7a1f11c 100644
--- a/arch/arm/mm/init.c
+++ b/arch/arm/mm/init.c
@@ -36,6 +36,7 @@
 
 #include "mm.h"
 
+
 static unsigned long phys_initrd_start __initdata = 0;
 static unsigned long phys_initrd_size __initdata = 0;
 
diff --git a/arch/arm/mm/mm.h b/arch/arm/mm/mm.h
index c1319e6..aebde8e 100644
--- a/arch/arm/mm/mm.h
+++ b/arch/arm/mm/mm.h
@@ -36,6 +36,27 @@ static inline pmd_t *pmd_off_k(unsigned long virt)
 	return pmd_offset(pud_offset(pgd_offset_k(virt), virt), virt);
 }
 
+static inline void set_fix_pte(unsigned long va, pte_t pte)
+{
+#if defined (CONFIG_ARM_PAGE_SIZE_LARGE) && defined(CONFIG_HIGHMEM)
+	pte_t *ptep = pte_offset_kernel(pmd_off_k(va), va);
+	set_pte_ext(ptep, pte, 0);
+	local_flush_tlb_kernel_page(va);
+#else
+	set_top_pte(va,pte);
+#endif
+}
+
+static inline pte_t get_fix_pte(unsigned long va)
+{
+#if defined (CONFIG_ARM_PAGE_SIZE_LARGE) && defined(CONFIG_HIGHMEM)
+	pte_t *ptep = pte_offset_kernel(pmd_off_k(va), va);
+	return *ptep;
+#else
+	return get_top_pte(va);
+#endif
+}
+
 struct mem_type {
 	pteval_t prot_pte;
 	pmdval_t prot_l1;
diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 555fc3a..b6b58c7 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -29,6 +29,10 @@
 #include <asm/system_info.h>
 #include <asm/traps.h>
 
+#if defined(CONFIG_ARM_PAGE_SIZE_LARGE) && defined(CONFIG_HIGHMEM)
+#include <asm/fixmap.h>
+#endif
+
 #include <asm/mach/arch.h>
 #include <asm/mach/map.h>
 #include <asm/mach/pci.h>
@@ -602,7 +606,9 @@ static void __init *early_alloc(unsigned long sz)
 static pte_t * __init early_pte_alloc(pmd_t *pmd, unsigned long addr, unsigned long prot)
 {
 	if (pmd_none(*pmd)) {
-		pte_t *pte = early_alloc(PTE_HWTABLE_OFF + PTE_HWTABLE_SIZE);
+		pte_t *pte = early_alloc(max_t(size_t,
+					PTE_HWTABLE_OFF + PTE_HWTABLE_SIZE,
+					PAGE_SIZE));
 		__pmd_populate(pmd, __pa(pte), prot);
 	}
 	BUG_ON(pmd_bad(*pmd));
@@ -1163,6 +1169,30 @@ void __init arm_mm_memblock_reserve(void)
 #endif
 }
 
+#if defined(CONFIG_ARM_PAGE_SIZE_LARGE) && defined(CONFIG_HIGHMEM)
+/* Prepare all levels for mapping highmem pages except the pte.
+ * This function isn't needed if FIXADDR is inside the already-existing
+ * mapping 0xfff0000 - 0xffffffff
+ * */
+static void __init prepare_highmem_tables(void)
+{
+	struct map_desc map;
+	unsigned long addr;
+
+	for (addr = FIXADDR_START; addr < FIXADDR_TOP; addr += SECTION_SIZE) {
+		/* map the first page from each section */
+		map.pfn = __phys_to_pfn(virt_to_phys((void *)addr));
+		map.virtual = addr;
+		map.length = PAGE_SIZE;
+		map.type = MT_MEMORY;
+		create_mapping(&map);
+
+		/* remove pte. Other pagetable levels are ready */
+		set_fix_pte(addr,__pte(0));
+	}
+}
+#endif /* CONFIG_ARM_PAGE_SIZE_LARGE && CONFIG_HIGHMEM */
+
 /*
  * Set up the device mappings.  Since we clear out the page tables for all
  * mappings above VMALLOC_START, we will remove any debug device mappings.
@@ -1245,6 +1275,10 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	map.type = MT_LOW_VECTORS;
 	create_mapping(&map);
 
+#if defined(CONFIG_ARM_PAGE_SIZE_LARGE) && defined(CONFIG_HIGHMEM)
+	prepare_highmem_tables();
+#endif
+
 	/*
 	 * Ask the machine support to map in the statically mapped devices.
 	 */
diff --git a/arch/arm/mm/proc-v7-3level.S b/arch/arm/mm/proc-v7-3level.S
index 6ba4bd9..bcb7d4b 100644
--- a/arch/arm/mm/proc-v7-3level.S
+++ b/arch/arm/mm/proc-v7-3level.S
@@ -61,6 +61,7 @@ ENDPROC(cpu_v7_switch_mm)
  *
  * Set a level 2 translation table entry.
  * - ptep - pointer to level 3 translation table entry
+ * -		(in r0, r1 - but r1 is always 0 because it's a virtual address)
  * - pte - PTE value to store (64-bit in r2 and r3)
  */
 ENTRY(cpu_v7_set_pte_ext)
@@ -72,10 +73,44 @@ ENTRY(cpu_v7_set_pte_ext)
 	bne	1f
 	tst	r3, #1 << (55 - 32)		@ L_PTE_DIRTY
 	orreq	r2, #L_PTE_RDONLY
-1:	strd	r2, r3, [r0]
+1:
+#if (HW_PAGES_PER_PAGE >= 2)
+	/* note: the first address is PAGE_SIZE-aligned
+	 * The difference between the first and last
+	 * address we map is one entry less than PAGE_SIZE
+	 * so we don't need to take carry into account
+	 */
+	add		r1, r0, #((HW_PAGES_PER_PAGE)<<3)
+	cmp		r2, #0
+	bne 3f
+	/* pte is zero, write 16 zeroes */
+2:	sub r1, r1, #8
+	strd	r2, r3, [r1]
+	ALT_SMP(W(nop))
+	ALT_UP (mcr	p15, 0, r1, c7, c10, 1)		@ flush_pte
+	cmp r1, r0
+	bgt 2b
+	b 5f
+3:	/* pte is not zero. Write 16 consecutive vals */
+#if (HW_PAGES_PER_PAGE >= 16)
+	orr		r3, r3, #1 << (52-32)	@contigous_bit
+#endif /* HW_PAGES_PER_PAGE >= 16 */
+	add		r2, r2, #(PAGE_SIZE)
+4:	sub r1, r1, #8
+	sub r2, r2, #(HW_PAGE_SIZE)
+	strd	r2, r3, [r1]
+	ALT_SMP(W(nop))
+	ALT_UP (mcr	p15, 0, r1, c7, c10, 1)		@ flush_pte
+	cmp r1, r0
+	bgt 4b
+5:	/*done looping */
+	mov r1, #0
+#else
+	strd	r2, r3, [r0]
 	ALT_SMP(W(nop))
 	ALT_UP (mcr	p15, 0, r0, c7, c10, 1)		@ flush_pte
-#endif
+#endif /* HW_PAGES_PER_PAGE >= 2 */
+#endif /*CONFIG_MMU*/
 	mov	pc, lr
 ENDPROC(cpu_v7_set_pte_ext)
 
diff --git a/arch/arm/mm/proc-v7.S b/arch/arm/mm/proc-v7.S
index 990d48c..b19626a 100644
--- a/arch/arm/mm/proc-v7.S
+++ b/arch/arm/mm/proc-v7.S
@@ -87,6 +87,7 @@ ENTRY(cpu_v7_dcache_clean_area)
 	mov	pc, lr
 ENDPROC(cpu_v7_dcache_clean_area)
 
+
 	string	cpu_v7_name, "ARMv7 Processor"
 	.align
 
diff --git a/arch/arm/mm/tlb-v7.S b/arch/arm/mm/tlb-v7.S
index ea94765..e8cd612 100644
--- a/arch/arm/mm/tlb-v7.S
+++ b/arch/arm/mm/tlb-v7.S
@@ -53,7 +53,7 @@ ENTRY(v7wbi_flush_user_tlb_range)
 #endif
 	ALT_UP(mcr	p15, 0, r0, c8, c7, 1)	@ TLB invalidate U MVA
 
-	add	r0, r0, #PAGE_SZ
+	add	r0, r0, #HW_PAGE_SIZE
 	cmp	r0, r1
 	blo	1b
 	dsb
@@ -81,7 +81,7 @@ ENTRY(v7wbi_flush_kern_tlb_range)
 	ALT_SMP(mcr	p15, 0, r0, c8, c3, 1)	@ TLB invalidate U MVA (shareable)
 #endif
 	ALT_UP(mcr	p15, 0, r0, c8, c7, 1)	@ TLB invalidate U MVA
-	add	r0, r0, #PAGE_SZ
+	add	r0, r0, #HW_PAGE_SIZE
 	cmp	r0, r1
 	blo	1b
 	dsb
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index fe120da..7d5c1d5 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -73,7 +73,7 @@ config X86
 	select HAVE_PERF_USER_STACK_DUMP
 	select HAVE_DEBUG_KMEMLEAK
 	select ANON_INODES
-	select HAVE_ALIGNED_STRUCT_PAGE if SLUB
+	select HAVE_ALIGNED_STRUCT_PAGE if SLUB && !LFS_ON_32CPU
 	select HAVE_CMPXCHG_LOCAL
 	select HAVE_CMPXCHG_DOUBLE
 	select HAVE_ARCH_KMEMCHECK
diff --git a/drivers/block/loop.c b/drivers/block/loop.c
index d92d50f..0f68c21 100644
--- a/drivers/block/loop.c
+++ b/drivers/block/loop.c
@@ -344,8 +344,8 @@ lo_splice_actor(struct pipe_inode_info *pipe, struct pipe_buffer *buf,
 		size = p->bsize;
 
 	if (lo_do_transfer(lo, READ, page, buf->offset, p->page, p->offset, size, IV)) {
-		printk(KERN_ERR "loop: transfer error block %ld\n",
-		       page->index);
+		printk(KERN_ERR "loop: transfer error block %lld\n",
+		       (unsigned long long)page->index);
 		size = -EINVAL;
 	}
 
diff --git a/drivers/md/bitmap.c b/drivers/md/bitmap.c
index 5a2c754..881972e 100644
--- a/drivers/md/bitmap.c
+++ b/drivers/md/bitmap.c
@@ -848,7 +848,7 @@ static void bitmap_file_set_bit(struct bitmap *bitmap, sector_t block)
 	else
 		set_bit_le(bit, kaddr);
 	kunmap_atomic(kaddr);
-	pr_debug("set file bit %lu page %lu\n", bit, page->index);
+	pr_debug("set file bit %lu page %llu\n", bit, (unsigned long long)page->index);
 	/* record page number so it gets flushed to disk when unplug occurs */
 	set_page_attr(bitmap, page->index, BITMAP_PAGE_DIRTY);
 }
diff --git a/drivers/pci/pci-sysfs.c b/drivers/pci/pci-sysfs.c
index 5b4a9d9..f8053af 100644
--- a/drivers/pci/pci-sysfs.c
+++ b/drivers/pci/pci-sysfs.c
@@ -936,8 +936,8 @@ pci_mmap_resource(struct kobject *kobj, struct bin_attribute *attr,
 
 	if (!pci_mmap_fits(pdev, i, vma, PCI_MMAP_SYSFS)) {
 		WARN(1, "process \"%s\" tried to map 0x%08lx bytes "
-			"at page 0x%08lx on %s BAR %d (start 0x%16Lx, size 0x%16Lx)\n",
-			current->comm, vma->vm_end-vma->vm_start, vma->vm_pgoff,
+			"at page 0x%016llx on %s BAR %d (start 0x%16llx, size 0x%16llx)\n",
+			current->comm, vma->vm_end-vma->vm_start, (unsigned long long)vma->vm_pgoff,
 			pci_name(pdev), i,
 			(u64)pci_resource_start(pdev, i),
 			(u64)pci_resource_len(pdev, i));
diff --git a/fs/Kconfig b/fs/Kconfig
index c229f82..b7b85bf 100644
--- a/fs/Kconfig
+++ b/fs/Kconfig
@@ -10,6 +10,16 @@ config DCACHE_WORD_ACCESS
 
 if BLOCK
 
+config LFS_ON_32CPU
+	bool "Support for large (16TB+) filesystems on 32-bit cpu"
+	depends on LBDAF
+	depends on !HUGETLBFS #hugetlbfs not supported yet
+	default n
+	help
+	  Enable support of running filesystem on block devices that are larger
+	  than 16TB on 32bit cpus
+
+
 source "fs/ext2/Kconfig"
 source "fs/ext3/Kconfig"
 source "fs/ext4/Kconfig"
diff --git a/fs/afs/rxrpc.c b/fs/afs/rxrpc.c
index 8ad8c2a..068678c 100644
--- a/fs/afs/rxrpc.c
+++ b/fs/afs/rxrpc.c
@@ -255,7 +255,7 @@ static int afs_send_pages(struct afs_call *call, struct msghdr *msg,
 	call->first_offset = 0;
 
 	do {
-		_debug("attach %lx-%lx", first, last);
+		_debug("attach %llx-%llx", (unsigned long long)first, (unsigned long long)last);
 
 		count = last - first + 1;
 		if (count > ARRAY_SIZE(pages))
diff --git a/fs/afs/vnode.c b/fs/afs/vnode.c
index 25cf4c3..c294fec 100644
--- a/fs/afs/vnode.c
+++ b/fs/afs/vnode.c
@@ -773,13 +773,13 @@ int afs_vnode_store_data(struct afs_writeback *wb, pgoff_t first, pgoff_t last,
 	struct afs_vnode *vnode = wb->vnode;
 	int ret;
 
-	_enter("%s{%x:%u.%u},%x,%lx,%lx,%x,%x",
+	_enter("%s{%x:%u.%u},%x,%llx,%llx,%x,%x",
 	       vnode->volume->vlocation->vldb.name,
 	       vnode->fid.vid,
 	       vnode->fid.vnode,
 	       vnode->fid.unique,
 	       key_serial(wb->key),
-	       first, last, offset, to);
+	       (unsigned long long)first, (unsigned long long)last, offset, to);
 
 	/* this op will fetch the status */
 	spin_lock(&vnode->lock);
diff --git a/fs/afs/write.c b/fs/afs/write.c
index a890db4..7ce035f 100644
--- a/fs/afs/write.c
+++ b/fs/afs/write.c
@@ -281,13 +281,13 @@ static void afs_kill_pages(struct afs_vnode *vnode, bool error,
 	struct pagevec pv;
 	unsigned count, loop;
 
-	_enter("{%x:%u},%lx-%lx",
-	       vnode->fid.vid, vnode->fid.vnode, first, last);
+	_enter("{%x:%u},%llx-%llx",
+	       vnode->fid.vid, vnode->fid.vnode, (unsigned long long)first, (unsigned long long)last);
 
 	pagevec_init(&pv, 0);
 
 	do {
-		_debug("kill %lx-%lx", first, last);
+		_debug("kill %llx-%llx", (unsigned long long)first, (unsigned long long)last);
 
 		count = last - first + 1;
 		if (count > PAGEVEC_SIZE)
diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c
index e7e7afb..2b212b1 100644
--- a/fs/btrfs/extent_io.c
+++ b/fs/btrfs/extent_io.c
@@ -3148,9 +3148,9 @@ static int __extent_writepage(struct page *page, struct writeback_control *wbc,
 
 			set_range_writeback(tree, cur, cur + iosize - 1);
 			if (!PageWriteback(page)) {
-				printk(KERN_ERR "btrfs warning page %lu not "
+				printk(KERN_ERR "btrfs warning page %llu not "
 				       "writeback, cur %llu end %llu\n",
-				       page->index, (unsigned long long)cur,
+				       (unsigned long long)page->index, (unsigned long long)cur,
 				       (unsigned long long)end);
 			}
 
diff --git a/fs/cifs/file.c b/fs/cifs/file.c
index c2934f8..1c3b70c 100644
--- a/fs/cifs/file.c
+++ b/fs/cifs/file.c
@@ -1884,7 +1884,7 @@ retry:
 		pgoff_t next = 0, tofind;
 		struct page **pages;
 
-		tofind = min((cifs_sb->wsize / PAGE_CACHE_SIZE) - 1,
+		tofind = min((pgoff_t)((cifs_sb->wsize / PAGE_CACHE_SIZE) - 1),
 				end - index) + 1;
 
 		wdata = cifs_writedata_alloc((unsigned int)tofind,
@@ -3142,7 +3142,7 @@ cifs_readpages_read_into_pages(struct TCP_Server_Info *server,
 	/* determine the eof that the server (probably) has */
 	eof = CIFS_I(rdata->mapping->host)->server_eof;
 	eof_index = eof ? (eof - 1) >> PAGE_CACHE_SHIFT : 0;
-	cifs_dbg(FYI, "eof=%llu eof_index=%lu\n", eof, eof_index);
+	cifs_dbg(FYI, "eof=%llu eof_index=%llu\n", eof, (unsigned long long)eof_index);
 
 	rdata->tailsz = PAGE_CACHE_SIZE;
 	for (i = 0; i < nr_pages; i++) {
diff --git a/fs/ecryptfs/crypto.c b/fs/ecryptfs/crypto.c
index dd640cf..08ef80f 100644
--- a/fs/ecryptfs/crypto.c
+++ b/fs/ecryptfs/crypto.c
@@ -456,8 +456,8 @@ static int ecryptfs_encrypt_extent(struct page *enc_extent_page,
 					  crypt_stat->extent_size, extent_iv);
 	if (rc < 0) {
 		printk(KERN_ERR "%s: Error attempting to encrypt page with "
-		       "page->index = [%ld], extent_offset = [%ld]; "
-		       "rc = [%d]\n", __func__, page->index, extent_offset,
+		       "page->index = [%lld], extent_offset = [%ld]; "
+		       "rc = [%d]\n", __func__, (unsigned long long)page->index, extent_offset,
 		       rc);
 		goto out;
 	}
@@ -564,8 +564,8 @@ static int ecryptfs_decrypt_extent(struct page *page,
 					  crypt_stat->extent_size, extent_iv);
 	if (rc < 0) {
 		printk(KERN_ERR "%s: Error attempting to decrypt to page with "
-		       "page->index = [%ld], extent_offset = [%ld]; "
-		       "rc = [%d]\n", __func__, page->index, extent_offset,
+		       "page->index = [%lld], extent_offset = [%ld]; "
+		       "rc = [%d]\n", __func__, (unsigned long long)page->index, extent_offset,
 		       rc);
 		goto out;
 	}
diff --git a/fs/ecryptfs/mmap.c b/fs/ecryptfs/mmap.c
index 564a1fa..53bec02 100644
--- a/fs/ecryptfs/mmap.c
+++ b/fs/ecryptfs/mmap.c
@@ -69,7 +69,7 @@ static int ecryptfs_writepage(struct page *page, struct writeback_control *wbc)
 	rc = ecryptfs_encrypt_page(page);
 	if (rc) {
 		ecryptfs_printk(KERN_WARNING, "Error encrypting "
-				"page (upper index [0x%.16lx])\n", page->index);
+				"page (upper index [0x%.16llx])\n", (unsigned long long)page->index);
 		ClearPageUptodate(page);
 		goto out;
 	}
@@ -237,8 +237,8 @@ out:
 		ClearPageUptodate(page);
 	else
 		SetPageUptodate(page);
-	ecryptfs_printk(KERN_DEBUG, "Unlocking page with index = [0x%.16lx]\n",
-			page->index);
+	ecryptfs_printk(KERN_DEBUG, "Unlocking page with index = [0x%.16llx]\n",
+			(unsigned long long)page->index);
 	unlock_page(page);
 	return rc;
 }
@@ -343,9 +343,9 @@ static int ecryptfs_write_begin(struct file *file,
 				rc = ecryptfs_decrypt_page(page);
 				if (rc) {
 					printk(KERN_ERR "%s: Error decrypting "
-					       "page at index [%ld]; "
+					       "page at index [%lld]; "
 					       "rc = [%d]\n",
-					       __func__, page->index, rc);
+					       __func__, (unsigned long long)page->index, rc);
 					ClearPageUptodate(page);
 					goto out;
 				}
@@ -489,7 +489,7 @@ static int ecryptfs_write_end(struct file *file,
 	int rc;
 
 	ecryptfs_printk(KERN_DEBUG, "Calling fill_zeros_to_end_of_page"
-			"(page w/ index = [0x%.16lx], to = [%d])\n", index, to);
+			"(page w/ index = [0x%.16llx], to = [%d])\n", (unsigned long long)index, to);
 	if (!(crypt_stat->flags & ECRYPTFS_ENCRYPTED)) {
 		rc = ecryptfs_write_lower_page_segment(ecryptfs_inode, page, 0,
 						       to);
@@ -511,13 +511,13 @@ static int ecryptfs_write_end(struct file *file,
 	rc = fill_zeros_to_end_of_page(page, to);
 	if (rc) {
 		ecryptfs_printk(KERN_WARNING, "Error attempting to fill "
-			"zeros in page with index = [0x%.16lx]\n", index);
+			"zeros in page with index = [0x%.16llxlx]\n", (unsigned long long)index);
 		goto out;
 	}
 	rc = ecryptfs_encrypt_page(page);
 	if (rc) {
 		ecryptfs_printk(KERN_WARNING, "Error encrypting page (upper "
-				"index [0x%.16lx])\n", index);
+				"index [0x%.16llx])\n", (unsigned long long)index);
 		goto out;
 	}
 	if (pos + copied > i_size_read(ecryptfs_inode)) {
diff --git a/fs/ecryptfs/read_write.c b/fs/ecryptfs/read_write.c
index 09fe622..435f848 100644
--- a/fs/ecryptfs/read_write.c
+++ b/fs/ecryptfs/read_write.c
@@ -147,9 +147,9 @@ int ecryptfs_write(struct inode *ecryptfs_inode, char *data, loff_t offset,
 		if (IS_ERR(ecryptfs_page)) {
 			rc = PTR_ERR(ecryptfs_page);
 			printk(KERN_ERR "%s: Error getting page at "
-			       "index [%ld] from eCryptfs inode "
+			       "index [%lld] from eCryptfs inode "
 			       "mapping; rc = [%d]\n", __func__,
-			       ecryptfs_page_idx, rc);
+			       (unsigned long long)ecryptfs_page_idx, rc);
 			goto out;
 		}
 		ecryptfs_page_virt = kmap_atomic(ecryptfs_page);
diff --git a/fs/exofs/inode.c b/fs/exofs/inode.c
index d1f80ab..439c618 100644
--- a/fs/exofs/inode.c
+++ b/fs/exofs/inode.c
@@ -577,7 +577,7 @@ static struct page *__r4w_get_page(void *priv, u64 offset, bool *uptodate)
 
 		if (offset >= i_size) {
 			*uptodate = true;
-			EXOFS_DBGMSG("offset >= i_size index=0x%lx\n", index);
+			EXOFS_DBGMSG("offset >= i_size index=0x%llx\n", _LLU(index));
 			return ZERO_PAGE(0);
 		}
 
@@ -596,7 +596,7 @@ static struct page *__r4w_get_page(void *priv, u64 offset, bool *uptodate)
 			*uptodate = true;
 		else
 			*uptodate = PageUptodate(page);
-		EXOFS_DBGMSG("index=0x%lx uptodate=%d\n", index, *uptodate);
+		EXOFS_DBGMSG("index=0x%llx uptodate=%d\n", _LLU(index), *uptodate);
 		return page;
 	} else {
 		EXOFS_DBGMSG("YES that_locked_page index=0x%lx\n",
diff --git a/fs/ext2/dir.c b/fs/ext2/dir.c
index 4237722bf..a880ed1 100644
--- a/fs/ext2/dir.c
+++ b/fs/ext2/dir.c
@@ -180,8 +180,8 @@ Einumber:
 bad_entry:
 	if (!quiet)
 		ext2_error(sb, __func__, "bad entry in directory #%lu: : %s - "
-			"offset=%lu, inode=%lu, rec_len=%d, name_len=%d",
-			dir->i_ino, error, (page->index<<PAGE_CACHE_SHIFT)+offs,
+			"offset=%llu, inode=%lu, rec_len=%d, name_len=%d",
+			dir->i_ino, error, (unsigned long long)(page->index<<PAGE_CACHE_SHIFT)+offs,
 			(unsigned long) le32_to_cpu(p->inode),
 			rec_len, p->name_len);
 	goto fail;
@@ -190,8 +190,8 @@ Eend:
 		p = (ext2_dirent *)(kaddr + offs);
 		ext2_error(sb, "ext2_check_page",
 			"entry in directory #%lu spans the page boundary"
-			"offset=%lu, inode=%lu",
-			dir->i_ino, (page->index<<PAGE_CACHE_SHIFT)+offs,
+			"offset=%llu, inode=%lu",
+			dir->i_ino, (unsigned long long)(page->index<<PAGE_CACHE_SHIFT)+offs,
 			(unsigned long) le32_to_cpu(p->inode));
 	}
 fail:
diff --git a/fs/jffs2/file.c b/fs/jffs2/file.c
index 1506673..2eec4c3 100644
--- a/fs/jffs2/file.c
+++ b/fs/jffs2/file.c
@@ -87,8 +87,8 @@ static int jffs2_do_readpage_nolock (struct inode *inode, struct page *pg)
 	unsigned char *pg_buf;
 	int ret;
 
-	jffs2_dbg(2, "%s(): ino #%lu, page at offset 0x%lx\n",
-		  __func__, inode->i_ino, pg->index << PAGE_CACHE_SHIFT);
+	jffs2_dbg(2, "%s(): ino #%lu, page at offset 0x%llx\n",
+		  __func__, inode->i_ino, (unsigned long long)pg->index << PAGE_CACHE_SHIFT);
 
 	BUG_ON(!PageLocked(pg));
 
@@ -255,8 +255,8 @@ static int jffs2_write_end(struct file *filp, struct address_space *mapping,
 	int ret = 0;
 	uint32_t writtenlen = 0;
 
-	jffs2_dbg(1, "%s(): ino #%lu, page at 0x%lx, range %d-%d, flags %lx\n",
-		  __func__, inode->i_ino, pg->index << PAGE_CACHE_SHIFT,
+	jffs2_dbg(1, "%s(): ino #%lu, page at 0x%llx, range %d-%d, flags %lx\n",
+		  __func__, inode->i_ino, (unsigned long long)pg->index << PAGE_CACHE_SHIFT,
 		  start, end, pg->flags);
 
 	/* We need to avoid deadlock with page_cache_read() in
diff --git a/fs/logfs/dir.c b/fs/logfs/dir.c
index b827510..c4d1e20 100644
--- a/fs/logfs/dir.c
+++ b/fs/logfs/dir.c
@@ -372,8 +372,8 @@ static struct dentry *logfs_lookup(struct inode *dir, struct dentry *dentry,
 
 	inode = logfs_iget(dir->i_sb, ino);
 	if (IS_ERR(inode))
-		printk(KERN_ERR"LogFS: Cannot read inode #%llx for dentry (%lx, %lx)n",
-				ino, dir->i_ino, index);
+		printk(KERN_ERR"LogFS: Cannot read inode #%llx for dentry (%lx, %llx)n",
+				ino, dir->i_ino, (unsigned long long)index);
 	return d_splice_alias(inode, dentry);
 }
 
diff --git a/fs/nfs/blocklayout/blocklayout.c b/fs/nfs/blocklayout/blocklayout.c
index 434b93e..9b94e4b 100644
--- a/fs/nfs/blocklayout/blocklayout.c
+++ b/fs/nfs/blocklayout/blocklayout.c
@@ -745,8 +745,8 @@ fill_invalid_ext:
 			}
 			/* page ref released in bl_end_io_write_zero */
 			index = isect >> PAGE_CACHE_SECTOR_SHIFT;
-			dprintk("%s zero %dth page: index %lu isect %llu\n",
-				__func__, npg_zero, index,
+			dprintk("%s zero %dth page: index %llu isect %llu\n",
+				__func__, npg_zero, (unsigned long long)index,
 				(unsigned long long)isect);
 			page = bl_find_get_zeroing_page(header->inode, index,
 							cow_read);
@@ -1219,7 +1219,8 @@ static u64 pnfs_num_cont_bytes(struct inode *inode, pgoff_t idx)
 	end = DIV_ROUND_UP(i_size_read(inode), PAGE_CACHE_SIZE);
 	if (end != NFS_I(inode)->npages) {
 		rcu_read_lock();
-		end = radix_tree_next_hole(&mapping->page_tree, idx + 1, ULONG_MAX);
+		end = radix_tree_next_hole(&mapping->page_tree, idx + 1,
+					   RDX_TREE_KEY_MAX_VALUE);
 		rcu_read_unlock();
 	}
 
diff --git a/fs/nfs/objlayout/objio_osd.c b/fs/nfs/objlayout/objio_osd.c
index 5457745..95b2aa2 100644
--- a/fs/nfs/objlayout/objio_osd.c
+++ b/fs/nfs/objlayout/objio_osd.c
@@ -495,7 +495,7 @@ static struct page *__r4w_get_page(void *priv, u64 offset, bool *uptodate)
 
 	if (offset >= i_size) {
 		*uptodate = true;
-		dprintk("%s: g_zero_page index=0x%lx\n", __func__, index);
+		dprintk("%s: g_zero_page index=0x%llx\n", __func__, (unsigned long long)index);
 		return ZERO_PAGE(0);
 	}
 
@@ -503,8 +503,8 @@ static struct page *__r4w_get_page(void *priv, u64 offset, bool *uptodate)
 	if (!page) {
 		page = find_or_create_page(mapping, index, GFP_NOFS);
 		if (unlikely(!page)) {
-			dprintk("%s: grab_cache_page Failed index=0x%lx\n",
-				__func__, index);
+			dprintk("%s: grab_cache_page Failed index=0x%llx\n",
+				__func__, (unsigned long long)index);
 			return NULL;
 		}
 		unlock_page(page);
@@ -513,7 +513,7 @@ static struct page *__r4w_get_page(void *priv, u64 offset, bool *uptodate)
 		*uptodate = true;
 	else
 		*uptodate = PageUptodate(page);
-	dprintk("%s: index=0x%lx uptodate=%d\n", __func__, index, *uptodate);
+	dprintk("%s: index=0x%llx uptodate=%d\n", __func__, (unsigned long long)index, *uptodate);
 	return page;
 }
 
diff --git a/fs/ubifs/debug.c b/fs/ubifs/debug.c
index 7f60e90..399ee22 100644
--- a/fs/ubifs/debug.c
+++ b/fs/ubifs/debug.c
@@ -267,8 +267,8 @@ void ubifs_dump_inode(struct ubifs_info *c, const struct inode *inode)
 	       (unsigned long long)ui->ui_size);
 	pr_err("\tflags          %d\n", ui->flags);
 	pr_err("\tcompr_type     %d\n", ui->compr_type);
-	pr_err("\tlast_page_read %lu\n", ui->last_page_read);
-	pr_err("\tread_in_a_row  %lu\n", ui->read_in_a_row);
+	pr_err("\tlast_page_read %llu\n", (unsigned long long)ui->last_page_read);
+	pr_err("\tread_in_a_row  %llu\n", (unsigned long long)ui->read_in_a_row);
 	pr_err("\tdata_len       %d\n", ui->data_len);
 
 	if (!S_ISDIR(inode->i_mode))
diff --git a/fs/ubifs/file.c b/fs/ubifs/file.c
index 348fd60..d6960df 100644
--- a/fs/ubifs/file.c
+++ b/fs/ubifs/file.c
@@ -112,8 +112,8 @@ static int do_readpage(struct page *page)
 	struct inode *inode = page->mapping->host;
 	loff_t i_size = i_size_read(inode);
 
-	dbg_gen("ino %lu, pg %lu, i_size %lld, flags %#lx",
-		inode->i_ino, page->index, i_size, page->flags);
+	dbg_gen("ino %lu, pg %llu, i_size %lld, flags %#lx",
+		inode->i_ino, (unsigned long long)page->index, i_size, page->flags);
 	ubifs_assert(!PageChecked(page));
 	ubifs_assert(!PagePrivate(page));
 
@@ -168,8 +168,8 @@ static int do_readpage(struct page *page)
 			dbg_gen("hole");
 			goto out_free;
 		}
-		ubifs_err("cannot read page %lu of inode %lu, error %d",
-			  page->index, inode->i_ino, err);
+		ubifs_err("cannot read page %llu of inode %lu, error %d",
+			  (unsigned long long)page->index, inode->i_ino, err);
 		goto error;
 	}
 
@@ -547,8 +547,8 @@ static int ubifs_write_end(struct file *file, struct address_space *mapping,
 	loff_t end_pos = pos + len;
 	int appending = !!(end_pos > inode->i_size);
 
-	dbg_gen("ino %lu, pos %llu, pg %lu, len %u, copied %d, i_size %lld",
-		inode->i_ino, pos, page->index, len, copied, inode->i_size);
+	dbg_gen("ino %lu, pos %llu, pg %llu, len %u, copied %d, i_size %lld",
+		inode->i_ino, pos, (unsigned long long)page->index, len, copied, inode->i_size);
 
 	if (unlikely(copied < len && len == PAGE_CACHE_SIZE)) {
 		/*
@@ -617,8 +617,8 @@ static int populate_page(struct ubifs_info *c, struct page *page,
 	void *addr, *zaddr;
 	pgoff_t end_index;
 
-	dbg_gen("ino %lu, pg %lu, i_size %lld, flags %#lx",
-		inode->i_ino, page->index, i_size, page->flags);
+	dbg_gen("ino %lu, pg %llu, i_size %lld, flags %#lx",
+		inode->i_ino, (unsigned long long)page->index, i_size, page->flags);
 
 	addr = zaddr = kmap(page);
 
@@ -929,8 +929,8 @@ static int do_writepage(struct page *page, int len)
 	}
 	if (err) {
 		SetPageError(page);
-		ubifs_err("cannot write page %lu of inode %lu, error %d",
-			  page->index, inode->i_ino, err);
+		ubifs_err("cannot write page %llu of inode %lu, error %d",
+			  (unsigned long long)page->index, inode->i_ino, err);
 		ubifs_ro_mode(c, err);
 	}
 
@@ -1005,8 +1005,8 @@ static int ubifs_writepage(struct page *page, struct writeback_control *wbc)
 	int err, len = i_size & (PAGE_CACHE_SIZE - 1);
 	void *kaddr;
 
-	dbg_gen("ino %lu, pg %lu, pg flags %#lx",
-		inode->i_ino, page->index, page->flags);
+	dbg_gen("ino %lu, pg %llu, pg flags %#lx",
+		inode->i_ino, (unsigned long long)page->index, page->flags);
 	ubifs_assert(PagePrivate(page));
 
 	/* Is the page fully outside @i_size? (truncate in progress) */
@@ -1444,7 +1444,7 @@ static int ubifs_vm_page_mkwrite(struct vm_area_struct *vma,
 	struct ubifs_budget_req req = { .new_page = 1 };
 	int err, update_time;
 
-	dbg_gen("ino %lu, pg %lu, i_size %lld",	inode->i_ino, page->index,
+	dbg_gen("ino %lu, pg %llu, i_size %lld", inode->i_ino, (unsigned long long)page->index,
 		i_size_read(inode));
 	ubifs_assert(!c->ro_media && !c->ro_mount);
 
diff --git a/fs/xfs/xfs_mount.c b/fs/xfs/xfs_mount.c
index e8e310c..49ba783 100644
--- a/fs/xfs/xfs_mount.c
+++ b/fs/xfs/xfs_mount.c
@@ -298,7 +298,7 @@ xfs_sb_validate_fsb_count(
 	ASSERT(sbp->sb_blocklog >= BBSHIFT);
 
 #if XFS_BIG_BLKNOS     /* Limited by ULONG_MAX of page cache index */
-	if (nblocks >> (PAGE_CACHE_SHIFT - sbp->sb_blocklog) > ULONG_MAX)
+	if (nblocks >> (PAGE_CACHE_SHIFT - sbp->sb_blocklog) > PGOFF_MAX)
 		return EFBIG;
 #else                  /* Limited by UINT_MAX of sectors */
 	if (nblocks << (sbp->sb_blocklog - BBSHIFT) > UINT_MAX)
diff --git a/fs/xfs/xfs_trace.h b/fs/xfs/xfs_trace.h
index aa4db33..6b31e16 100644
--- a/fs/xfs/xfs_trace.h
+++ b/fs/xfs/xfs_trace.h
@@ -998,7 +998,7 @@ DECLARE_EVENT_CLASS(xfs_page_class,
 		__entry->delalloc = delalloc;
 		__entry->unwritten = unwritten;
 	),
-	TP_printk("dev %d:%d ino 0x%llx pgoff 0x%lx size 0x%llx offset %lx "
+	TP_printk("dev %d:%d ino 0x%llx pgoff 0x%llx size 0x%llx offset %lx "
 		  "delalloc %d unwritten %d",
 		  MAJOR(__entry->dev), MINOR(__entry->dev),
 		  __entry->ino,
diff --git a/include/linux/mm.h b/include/linux/mm.h
index c5cd5db..3a6c8ea 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1455,9 +1455,9 @@ void vma_interval_tree_insert_after(struct vm_area_struct *node,
 void vma_interval_tree_remove(struct vm_area_struct *node,
 			      struct rb_root *root);
 struct vm_area_struct *vma_interval_tree_iter_first(struct rb_root *root,
-				unsigned long start, unsigned long last);
+				 pgoff_t start, pgoff_t last);
 struct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,
-				unsigned long start, unsigned long last);
+				pgoff_t start, pgoff_t last);
 
 #define vma_interval_tree_foreach(vma, root, start, last)		\
 	for (vma = vma_interval_tree_iter_first(root, start, last);	\
@@ -1474,9 +1474,9 @@ void anon_vma_interval_tree_insert(struct anon_vma_chain *node,
 void anon_vma_interval_tree_remove(struct anon_vma_chain *node,
 				   struct rb_root *root);
 struct anon_vma_chain *anon_vma_interval_tree_iter_first(
-	struct rb_root *root, unsigned long start, unsigned long last);
+	struct rb_root *root, pgoff_t start, pgoff_t last);
 struct anon_vma_chain *anon_vma_interval_tree_iter_next(
-	struct anon_vma_chain *node, unsigned long start, unsigned long last);
+	struct anon_vma_chain *node, pgoff_t start, pgoff_t last);
 #ifdef CONFIG_DEBUG_VM_RB
 void anon_vma_interval_tree_verify(struct anon_vma_chain *node);
 #endif
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 4a189ba..1863223 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -278,7 +278,7 @@ struct vm_area_struct {
 	const struct vm_operations_struct *vm_ops;
 
 	/* Information about our backing store: */
-	unsigned long vm_pgoff;		/* Offset (within vm_file) in PAGE_SIZE
+	pgoff_t vm_pgoff;		/* Offset (within vm_file) in PAGE_SIZE
 					   units, *not* PAGE_CACHE_SIZE */
 	struct file * vm_file;		/* File we map to (can be NULL). */
 	void * vm_private_data;		/* was vm_pte (shared mem) */
diff --git a/include/linux/radix-tree.h b/include/linux/radix-tree.h
index ffc444c..6c0cc25 100644
--- a/include/linux/radix-tree.h
+++ b/include/linux/radix-tree.h
@@ -27,6 +27,14 @@
 #include <linux/kernel.h>
 #include <linux/rcupdate.h>
 
+/* index type */
+#ifdef CONFIG_LFS_ON_32CPU
+#define rdx_t	unsigned long long
+#define RDX_TREE_KEY_MAX_VALUE	ULLONG_MAX
+#else
+#define rdx_t	unsigned long
+#define RDX_TREE_KEY_MAX_VALUE	ULONG_MAX
+#endif
 /*
  * An indirect pointer (root->rnode pointing to a radix_tree_node, rather
  * than a data item) is signalled by the low bit set in the root->rnode
@@ -216,42 +224,42 @@ static inline void radix_tree_replace_slot(void **pslot, void *item)
 	rcu_assign_pointer(*pslot, item);
 }
 
-int radix_tree_insert(struct radix_tree_root *, unsigned long, void *);
-void *radix_tree_lookup(struct radix_tree_root *, unsigned long);
-void **radix_tree_lookup_slot(struct radix_tree_root *, unsigned long);
-void *radix_tree_delete(struct radix_tree_root *, unsigned long);
+int radix_tree_insert(struct radix_tree_root *, rdx_t, void *);
+void *radix_tree_lookup(struct radix_tree_root *, rdx_t);
+void **radix_tree_lookup_slot(struct radix_tree_root *, rdx_t);
+void *radix_tree_delete(struct radix_tree_root *, rdx_t);
 unsigned int
 radix_tree_gang_lookup(struct radix_tree_root *root, void **results,
-			unsigned long first_index, unsigned int max_items);
+			rdx_t first_index, unsigned int max_items);
 unsigned int radix_tree_gang_lookup_slot(struct radix_tree_root *root,
-			void ***results, unsigned long *indices,
-			unsigned long first_index, unsigned int max_items);
-unsigned long radix_tree_next_hole(struct radix_tree_root *root,
-				unsigned long index, unsigned long max_scan);
-unsigned long radix_tree_prev_hole(struct radix_tree_root *root,
-				unsigned long index, unsigned long max_scan);
+			void ***results, rdx_t *indices,
+			rdx_t first_index, unsigned int max_items);
+rdx_t radix_tree_next_hole(struct radix_tree_root *root,
+				rdx_t index, rdx_t max_scan);
+rdx_t radix_tree_prev_hole(struct radix_tree_root *root,
+				rdx_t index, rdx_t max_scan);
 int radix_tree_preload(gfp_t gfp_mask);
 void radix_tree_init(void);
 void *radix_tree_tag_set(struct radix_tree_root *root,
-			unsigned long index, unsigned int tag);
+			rdx_t index, unsigned int tag);
 void *radix_tree_tag_clear(struct radix_tree_root *root,
-			unsigned long index, unsigned int tag);
+			rdx_t index, unsigned int tag);
 int radix_tree_tag_get(struct radix_tree_root *root,
-			unsigned long index, unsigned int tag);
+			rdx_t index, unsigned int tag);
 unsigned int
 radix_tree_gang_lookup_tag(struct radix_tree_root *root, void **results,
-		unsigned long first_index, unsigned int max_items,
+		rdx_t first_index, unsigned int max_items,
 		unsigned int tag);
 unsigned int
 radix_tree_gang_lookup_tag_slot(struct radix_tree_root *root, void ***results,
-		unsigned long first_index, unsigned int max_items,
+		rdx_t first_index, unsigned int max_items,
 		unsigned int tag);
 unsigned long radix_tree_range_tag_if_tagged(struct radix_tree_root *root,
-		unsigned long *first_indexp, unsigned long last_index,
+		rdx_t *first_indexp, rdx_t last_index,
 		unsigned long nr_to_tag,
 		unsigned int fromtag, unsigned int totag);
 int radix_tree_tagged(struct radix_tree_root *root, unsigned int tag);
-unsigned long radix_tree_locate_item(struct radix_tree_root *root, void *item);
+rdx_t radix_tree_locate_item(struct radix_tree_root *root, void *item);
 
 static inline void radix_tree_preload_end(void)
 {
@@ -273,8 +281,8 @@ static inline void radix_tree_preload_end(void)
  * radix tree tag.
  */
 struct radix_tree_iter {
-	unsigned long	index;
-	unsigned long	next_index;
+	rdx_t	index;
+	rdx_t	next_index;
 	unsigned long	tags;
 };
 
@@ -290,7 +298,7 @@ struct radix_tree_iter {
  * Returns:	NULL
  */
 static __always_inline void **
-radix_tree_iter_init(struct radix_tree_iter *iter, unsigned long start)
+radix_tree_iter_init(struct radix_tree_iter *iter, rdx_t start)
 {
 	/*
 	 * Leave iter->tags uninitialized. radix_tree_next_chunk() will fill it
diff --git a/include/linux/rmap.h b/include/linux/rmap.h
index 6dacb93..bcd1fe2 100644
--- a/include/linux/rmap.h
+++ b/include/linux/rmap.h
@@ -64,10 +64,10 @@ struct anon_vma_chain {
 	struct vm_area_struct *vma;
 	struct anon_vma *anon_vma;
 	struct list_head same_vma;   /* locked by mmap_sem & page_table_lock */
-	struct rb_node rb;			/* locked by anon_vma->rwsem */
-	unsigned long rb_subtree_last;
+	struct rb_node rb;			/* locked by anon_vma->mutex */
+	pgoff_t rb_subtree_last;
 #ifdef CONFIG_DEBUG_VM_RB
-	unsigned long cached_vma_start, cached_vma_last;
+	pgoff_t cached_vma_start, cached_vma_last;
 #endif
 };
 
diff --git a/include/linux/types.h b/include/linux/types.h
index 4d118ba..577c8cd 100644
--- a/include/linux/types.h
+++ b/include/linux/types.h
@@ -139,7 +139,13 @@ typedef unsigned long blkcnt_t;
  * can override it.
  */
 #ifndef pgoff_t
+#ifdef CONFIG_LFS_ON_32CPU
+#define pgoff_t unsigned long long
+#define PGOFF_MAX	ULLONG_MAX
+#else
 #define pgoff_t unsigned long
+#define PGOFF_MAX	ULONG_MAX
+#endif
 #endif
 
 #ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
diff --git a/include/trace/events/btrfs.h b/include/trace/events/btrfs.h
index ea546a4..912f01b 100644
--- a/include/trace/events/btrfs.h
+++ b/include/trace/events/btrfs.h
@@ -321,17 +321,17 @@ DECLARE_EVENT_CLASS(btrfs__writepage,
 				 BTRFS_I(inode)->root->root_key.objectid;
 	),
 
-	TP_printk("root = %llu(%s), ino = %lu, page_index = %lu, "
+	TP_printk("root = %llu(%s), ino = %lu, page_index = %llu, "
 		  "nr_to_write = %ld, pages_skipped = %ld, range_start = %llu, "
 		  "range_end = %llu, for_kupdate = %d, "
-		  "for_reclaim = %d, range_cyclic = %d, writeback_index = %lu",
+		  "for_reclaim = %d, range_cyclic = %d, writeback_index = %llu",
 		  show_root_type(__entry->root_objectid),
-		  (unsigned long)__entry->ino, __entry->index,
+		  (unsigned long)__entry->ino, (unsigned long long)__entry->index,
 		  __entry->nr_to_write, __entry->pages_skipped,
 		  __entry->range_start, __entry->range_end,
 		  __entry->for_kupdate,
 		  __entry->for_reclaim, __entry->range_cyclic,
-		  (unsigned long)__entry->writeback_index)
+		  (unsigned long long)__entry->writeback_index)
 );
 
 DEFINE_EVENT(btrfs__writepage, __extent_writepage,
diff --git a/include/trace/events/ext3.h b/include/trace/events/ext3.h
index 15d11a3..4068ba4 100644
--- a/include/trace/events/ext3.h
+++ b/include/trace/events/ext3.h
@@ -249,9 +249,9 @@ DECLARE_EVENT_CLASS(ext3__page_op,
 		__entry->dev	= page->mapping->host->i_sb->s_dev;
 	),
 
-	TP_printk("dev %d,%d ino %lu page_index %lu",
+	TP_printk("dev %d,%d ino %lu page_index %llu",
 		  MAJOR(__entry->dev), MINOR(__entry->dev),
-		  (unsigned long) __entry->ino, __entry->index)
+		  (unsigned long) __entry->ino, (unsigned long long)__entry->index)
 );
 
 DEFINE_EVENT(ext3__page_op, ext3_ordered_writepage,
@@ -309,10 +309,10 @@ TRACE_EVENT(ext3_invalidatepage,
 		__entry->dev	= page->mapping->host->i_sb->s_dev;
 	),
 
-	TP_printk("dev %d,%d ino %lu page_index %lu offset %lu",
+	TP_printk("dev %d,%d ino %lu page_index %llu offset %lu",
 		  MAJOR(__entry->dev), MINOR(__entry->dev),
 		  (unsigned long) __entry->ino,
-		  __entry->index, __entry->offset)
+		  (unsigned long long)__entry->index, __entry->offset)
 );
 
 TRACE_EVENT(ext3_discard_blocks,
diff --git a/include/trace/events/writeback.h b/include/trace/events/writeback.h
index 464ea82..ae2e6d6 100644
--- a/include/trace/events/writeback.h
+++ b/include/trace/events/writeback.h
@@ -51,10 +51,10 @@ TRACE_EVENT(writeback_dirty_page,
 		__entry->index = page->index;
 	),
 
-	TP_printk("bdi %s: ino=%lu index=%lu",
+	TP_printk("bdi %s: ino=%lu index=%llu",
 		__entry->name,
 		__entry->ino,
-		__entry->index
+		(unsigned long long)__entry->index
 	)
 );
 
diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c
index ad8e1bd..9a329ec 100644
--- a/kernel/events/uprobes.c
+++ b/kernel/events/uprobes.c
@@ -685,7 +685,7 @@ static inline struct map_info *free_map_info(struct map_info *info)
 static struct map_info *
 build_map_info(struct address_space *mapping, loff_t offset, bool is_register)
 {
-	unsigned long pgoff = offset >> PAGE_SHIFT;
+	pgoff_t pgoff = offset >> PAGE_SHIFT;
 	struct vm_area_struct *vma;
 	struct map_info *curr = NULL;
 	struct map_info *prev = NULL;
diff --git a/lib/radix-tree.c b/lib/radix-tree.c
index e796429..86de0b2 100644
--- a/lib/radix-tree.c
+++ b/lib/radix-tree.c
@@ -33,6 +33,13 @@
 #include <linux/bitops.h>
 #include <linux/rcupdate.h>
 
+#ifdef CONFIG_LFS_ON_32CPU
+#define RADIX_TREE_1	1ULL
+#define RADIX_TREE_BITS_PER_KEY		64
+#else
+#define RADIX_TREE_1	1UL
+#define RADIX_TREE_BITS_PER_KEY		BITS_PER_LONG
+#endif
 
 #ifdef __KERNEL__
 #define RADIX_TREE_MAP_SHIFT	(CONFIG_BASE_SMALL ? 4 : 6)
@@ -40,7 +47,7 @@
 #define RADIX_TREE_MAP_SHIFT	3	/* For more stressful testing */
 #endif
 
-#define RADIX_TREE_MAP_SIZE	(1UL << RADIX_TREE_MAP_SHIFT)
+#define RADIX_TREE_MAP_SIZE	(RADIX_TREE_1 << RADIX_TREE_MAP_SHIFT)
 #define RADIX_TREE_MAP_MASK	(RADIX_TREE_MAP_SIZE-1)
 
 #define RADIX_TREE_TAG_LONGS	\
@@ -57,7 +64,7 @@ struct radix_tree_node {
 	unsigned long	tags[RADIX_TREE_MAX_TAGS][RADIX_TREE_TAG_LONGS];
 };
 
-#define RADIX_TREE_INDEX_BITS  (8 /* CHAR_BIT */ * sizeof(unsigned long))
+#define RADIX_TREE_INDEX_BITS  (8 /* CHAR_BIT */ * sizeof(rdx_t))
 #define RADIX_TREE_MAX_PATH (DIV_ROUND_UP(RADIX_TREE_INDEX_BITS, \
 					  RADIX_TREE_MAP_SHIFT))
 
@@ -65,7 +72,7 @@ struct radix_tree_node {
  * The height_to_maxindex array needs to be one deeper than the maximum
  * path as height 0 holds only 1 entry.
  */
-static unsigned long height_to_maxindex[RADIX_TREE_MAX_PATH + 1] __read_mostly;
+static rdx_t height_to_maxindex[RADIX_TREE_MAX_PATH + 1] __read_mostly;
 
 /*
  * Radix tree node cache.
@@ -79,9 +86,9 @@ static struct kmem_cache *radix_tree_node_cachep;
  * radix_tree_extend).
  *
  * The worst case is a zero height tree with just a single item at index 0,
- * and then inserting an item at index ULONG_MAX. This requires 2 new branches
- * of RADIX_TREE_MAX_PATH size to be created, with only the root node shared.
- * Hence:
+ * and then inserting an item at index RDX_TREE_KEY_MAX_VALUE. This requires 2
+ * new branches of RADIX_TREE_MAX_PATH size to be created, with only the root
+ * node shared. Hence:
  */
 #define RADIX_TREE_PRELOAD_SIZE (RADIX_TREE_MAX_PATH * 2 - 1)
 
@@ -294,7 +301,7 @@ EXPORT_SYMBOL(radix_tree_preload);
  *	Return the maximum key which can be store into a
  *	radix tree with height HEIGHT.
  */
-static inline unsigned long radix_tree_maxindex(unsigned int height)
+static inline rdx_t radix_tree_maxindex(unsigned int height)
 {
 	return height_to_maxindex[height];
 }
@@ -302,7 +309,7 @@ static inline unsigned long radix_tree_maxindex(unsigned int height)
 /*
  *	Extend a radix tree so it can store key @index.
  */
-static int radix_tree_extend(struct radix_tree_root *root, unsigned long index)
+static int radix_tree_extend(struct radix_tree_root *root, rdx_t index)
 {
 	struct radix_tree_node *node;
 	struct radix_tree_node *slot;
@@ -358,7 +365,7 @@ out:
  *	Insert an item into the radix tree at position @index.
  */
 int radix_tree_insert(struct radix_tree_root *root,
-			unsigned long index, void *item)
+			rdx_t index, void *item)
 {
 	struct radix_tree_node *node = NULL, *slot;
 	unsigned int height, shift;
@@ -425,7 +432,7 @@ EXPORT_SYMBOL(radix_tree_insert);
  * is_slot == 0 : search for the node.
  */
 static void *radix_tree_lookup_element(struct radix_tree_root *root,
-				unsigned long index, int is_slot)
+				rdx_t index, int is_slot)
 {
 	unsigned int height, shift;
 	struct radix_tree_node *node, **slot;
@@ -474,7 +481,7 @@ static void *radix_tree_lookup_element(struct radix_tree_root *root,
  *	exclusive from other writers. Any dereference of the slot must be done
  *	using radix_tree_deref_slot.
  */
-void **radix_tree_lookup_slot(struct radix_tree_root *root, unsigned long index)
+void **radix_tree_lookup_slot(struct radix_tree_root *root, rdx_t index)
 {
 	return (void **)radix_tree_lookup_element(root, index, 1);
 }
@@ -492,7 +499,7 @@ EXPORT_SYMBOL(radix_tree_lookup_slot);
  *	them safely). No RCU barriers are required to access or modify the
  *	returned item, however.
  */
-void *radix_tree_lookup(struct radix_tree_root *root, unsigned long index)
+void *radix_tree_lookup(struct radix_tree_root *root, rdx_t index)
 {
 	return radix_tree_lookup_element(root, index, 0);
 }
@@ -512,7 +519,7 @@ EXPORT_SYMBOL(radix_tree_lookup);
  *	item is a bug.
  */
 void *radix_tree_tag_set(struct radix_tree_root *root,
-			unsigned long index, unsigned int tag)
+			rdx_t index, unsigned int tag)
 {
 	unsigned int height, shift;
 	struct radix_tree_node *slot;
@@ -558,7 +565,7 @@ EXPORT_SYMBOL(radix_tree_tag_set);
  *	has the same return value and semantics as radix_tree_lookup().
  */
 void *radix_tree_tag_clear(struct radix_tree_root *root,
-			unsigned long index, unsigned int tag)
+			rdx_t index, unsigned int tag)
 {
 	struct radix_tree_node *node = NULL;
 	struct radix_tree_node *slot = NULL;
@@ -622,7 +629,7 @@ EXPORT_SYMBOL(radix_tree_tag_clear);
  * from concurrency.
  */
 int radix_tree_tag_get(struct radix_tree_root *root,
-			unsigned long index, unsigned int tag)
+			rdx_t index, unsigned int tag)
 {
 	unsigned int height, shift;
 	struct radix_tree_node *node;
@@ -676,7 +683,7 @@ void **radix_tree_next_chunk(struct radix_tree_root *root,
 {
 	unsigned shift, tag = flags & RADIX_TREE_ITER_TAG_MASK;
 	struct radix_tree_node *rnode, *node;
-	unsigned long index, offset;
+	rdx_t index, offset;
 
 	if ((flags & RADIX_TREE_ITER_TAGGED) && !root_tag_get(root, tag))
 		return NULL;
@@ -803,11 +810,11 @@ EXPORT_SYMBOL(radix_tree_next_chunk);
  *
  * The function returns number of leaves where the tag was set and sets
  * *first_indexp to the first unscanned index.
- * WARNING! *first_indexp can wrap if last_index is ULONG_MAX. Caller must
- * be prepared to handle that.
+ * WARNING! *first_indexp can wrap if last_index is RDX_TREE_KEY_MAX_VALUE.
+ * Caller must be prepared to handle that.
  */
 unsigned long radix_tree_range_tag_if_tagged(struct radix_tree_root *root,
-		unsigned long *first_indexp, unsigned long last_index,
+		rdx_t *first_indexp, rdx_t last_index,
 		unsigned long nr_to_tag,
 		unsigned int iftag, unsigned int settag)
 {
@@ -816,7 +823,7 @@ unsigned long radix_tree_range_tag_if_tagged(struct radix_tree_root *root,
 	struct radix_tree_node *slot;
 	unsigned int shift;
 	unsigned long tagged = 0;
-	unsigned long index = *first_indexp;
+	rdx_t index = *first_indexp;
 
 	last_index = min(last_index, radix_tree_maxindex(height));
 	if (index > last_index)
@@ -837,7 +844,7 @@ unsigned long radix_tree_range_tag_if_tagged(struct radix_tree_root *root,
 	slot = indirect_to_ptr(root->rnode);
 
 	for (;;) {
-		unsigned long upindex;
+		rdx_t upindex;
 		int offset;
 
 		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
@@ -930,10 +937,10 @@ EXPORT_SYMBOL(radix_tree_range_tag_if_tagged);
  *	radix_tree_next_hole covering both indexes may return 10 if called
  *	under rcu_read_lock.
  */
-unsigned long radix_tree_next_hole(struct radix_tree_root *root,
-				unsigned long index, unsigned long max_scan)
+rdx_t radix_tree_next_hole(struct radix_tree_root *root,
+				rdx_t index, rdx_t max_scan)
 {
-	unsigned long i;
+	rdx_t i;
 
 	for (i = 0; i < max_scan; i++) {
 		if (!radix_tree_lookup(root, index))
@@ -958,7 +965,8 @@ EXPORT_SYMBOL(radix_tree_next_hole);
  *
  *	Returns: the index of the hole if found, otherwise returns an index
  *	outside of the set specified (in which case 'index - return >= max_scan'
- *	will be true). In rare cases of wrap-around, ULONG_MAX will be returned.
+ *	will be true). In rare cases of wrap-around, RDX_TREE_KEY_MAX_VALUE
+ *	will be returned.
  *
  *	radix_tree_next_hole may be called under rcu_read_lock. However, like
  *	radix_tree_gang_lookup, this will not atomically search a snapshot of
@@ -967,16 +975,16 @@ EXPORT_SYMBOL(radix_tree_next_hole);
  *	radix_tree_prev_hole covering both indexes may return 5 if called under
  *	rcu_read_lock.
  */
-unsigned long radix_tree_prev_hole(struct radix_tree_root *root,
-				   unsigned long index, unsigned long max_scan)
+rdx_t radix_tree_prev_hole(struct radix_tree_root *root,
+				   rdx_t index, rdx_t max_scan)
 {
-	unsigned long i;
+	rdx_t i;
 
 	for (i = 0; i < max_scan; i++) {
 		if (!radix_tree_lookup(root, index))
 			break;
 		index--;
-		if (index == ULONG_MAX)
+		if (index == RDX_TREE_KEY_MAX_VALUE)
 			break;
 	}
 
@@ -1005,7 +1013,7 @@ EXPORT_SYMBOL(radix_tree_prev_hole);
  */
 unsigned int
 radix_tree_gang_lookup(struct radix_tree_root *root, void **results,
-			unsigned long first_index, unsigned int max_items)
+			rdx_t first_index, unsigned int max_items)
 {
 	struct radix_tree_iter iter;
 	void **slot;
@@ -1046,8 +1054,8 @@ EXPORT_SYMBOL(radix_tree_gang_lookup);
  */
 unsigned int
 radix_tree_gang_lookup_slot(struct radix_tree_root *root,
-			void ***results, unsigned long *indices,
-			unsigned long first_index, unsigned int max_items)
+			void ***results, rdx_t *indices,
+			rdx_t first_index, unsigned int max_items)
 {
 	struct radix_tree_iter iter;
 	void **slot;
@@ -1083,7 +1091,7 @@ EXPORT_SYMBOL(radix_tree_gang_lookup_slot);
  */
 unsigned int
 radix_tree_gang_lookup_tag(struct radix_tree_root *root, void **results,
-		unsigned long first_index, unsigned int max_items,
+		rdx_t first_index, unsigned int max_items,
 		unsigned int tag)
 {
 	struct radix_tree_iter iter;
@@ -1120,7 +1128,7 @@ EXPORT_SYMBOL(radix_tree_gang_lookup_tag);
  */
 unsigned int
 radix_tree_gang_lookup_tag_slot(struct radix_tree_root *root, void ***results,
-		unsigned long first_index, unsigned int max_items,
+		rdx_t first_index, unsigned int max_items,
 		unsigned int tag)
 {
 	struct radix_tree_iter iter;
@@ -1146,11 +1154,11 @@ EXPORT_SYMBOL(radix_tree_gang_lookup_tag_slot);
 /*
  * This linear search is at present only useful to shmem_unuse_inode().
  */
-static unsigned long __locate(struct radix_tree_node *slot, void *item,
-			      unsigned long index, unsigned long *found_index)
+static rdx_t __locate(struct radix_tree_node *slot, void *item,
+			      rdx_t index, rdx_t *found_index)
 {
 	unsigned int shift, height;
-	unsigned long i;
+	rdx_t i;
 
 	height = slot->height;
 	shift = (height-1) * RADIX_TREE_MAP_SHIFT;
@@ -1160,8 +1168,8 @@ static unsigned long __locate(struct radix_tree_node *slot, void *item,
 		for (;;) {
 			if (slot->slots[i] != NULL)
 				break;
-			index &= ~((1UL << shift) - 1);
-			index += 1UL << shift;
+			index &= ~((RADIX_TREE_1 << shift) - 1);
+			index += RADIX_TREE_1 << shift;
 			if (index == 0)
 				goto out;	/* 32-bit wraparound */
 			i++;
@@ -1197,12 +1205,12 @@ out:
  *	Caller must hold no lock (since this time-consuming function needs
  *	to be preemptible), and must check afterwards if item is still there.
  */
-unsigned long radix_tree_locate_item(struct radix_tree_root *root, void *item)
+rdx_t radix_tree_locate_item(struct radix_tree_root *root, void *item)
 {
 	struct radix_tree_node *node;
-	unsigned long max_index;
-	unsigned long cur_index = 0;
-	unsigned long found_index = -1;
+	rdx_t max_index;
+	rdx_t cur_index = 0;
+	rdx_t found_index = -1;
 
 	do {
 		rcu_read_lock();
@@ -1227,7 +1235,7 @@ unsigned long radix_tree_locate_item(struct radix_tree_root *root, void *item)
 	return found_index;
 }
 #else
-unsigned long radix_tree_locate_item(struct radix_tree_root *root, void *item)
+rdx_t radix_tree_locate_item(struct radix_tree_root *root, void *item)
 {
 	return -1;
 }
@@ -1306,7 +1314,7 @@ static inline void radix_tree_shrink(struct radix_tree_root *root)
  *
  *	Returns the address of the deleted item, or NULL if it was not present.
  */
-void *radix_tree_delete(struct radix_tree_root *root, unsigned long index)
+void *radix_tree_delete(struct radix_tree_root *root, rdx_t index)
 {
 	struct radix_tree_node *node = NULL;
 	struct radix_tree_node *slot = NULL;
@@ -1404,16 +1412,16 @@ radix_tree_node_ctor(void *node)
 	memset(node, 0, sizeof(struct radix_tree_node));
 }
 
-static __init unsigned long __maxindex(unsigned int height)
+static __init rdx_t __maxindex(unsigned int height)
 {
 	unsigned int width = height * RADIX_TREE_MAP_SHIFT;
 	int shift = RADIX_TREE_INDEX_BITS - width;
 
 	if (shift < 0)
-		return ~0UL;
-	if (shift >= BITS_PER_LONG)
-		return 0UL;
-	return ~0UL >> shift;
+		return RDX_TREE_KEY_MAX_VALUE;
+	if (shift >= RADIX_TREE_BITS_PER_KEY)
+		return (rdx_t)0;
+	return RDX_TREE_KEY_MAX_VALUE >> shift;
 }
 
 static __init void radix_tree_init_maxindex(void)
diff --git a/mm/filemap.c b/mm/filemap.c
index 7905fe7..cb91cb7 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -1565,7 +1565,11 @@ static void do_sync_mmap_readahead(struct vm_area_struct *vma,
 	 * mmap read-around
 	 */
 	ra_pages = max_sane_readahead(ra->ra_pages);
+#ifdef CONFIG_LFS_ON_32CPU
+	ra->start = max_t(long long, 0, offset - ra_pages / 2);
+#else
 	ra->start = max_t(long, 0, offset - ra_pages / 2);
+#endif
 	ra->size = ra_pages;
 	ra->async_size = ra_pages / 4;
 	ra_submit(ra, mapping, file);
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 7c5eb85..4180115 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -2236,8 +2236,8 @@ static void hugetlb_vm_op_close(struct vm_area_struct *vma)
 	struct resv_map *reservations = vma_resv_map(vma);
 	struct hugepage_subpool *spool = subpool_vma(vma);
 	unsigned long reserve;
-	unsigned long start;
-	unsigned long end;
+	pgoff_t start;
+	pgoff_t end;
 
 	if (reservations) {
 		start = vma_hugecache_offset(h, vma, vma->vm_start);
@@ -2715,7 +2715,7 @@ static int hugetlb_no_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	int ret = VM_FAULT_SIGBUS;
 	int anon_rmap = 0;
 	pgoff_t idx;
-	unsigned long size;
+	loff_t size;
 	struct page *page;
 	struct address_space *mapping;
 	pte_t new_pte;
diff --git a/mm/interval_tree.c b/mm/interval_tree.c
index 4a5822a..ebfa277 100644
--- a/mm/interval_tree.c
+++ b/mm/interval_tree.c
@@ -11,18 +11,18 @@
 #include <linux/rmap.h>
 #include <linux/interval_tree_generic.h>
 
-static inline unsigned long vma_start_pgoff(struct vm_area_struct *v)
+static inline pgoff_t vma_start_pgoff(struct vm_area_struct *v)
 {
 	return v->vm_pgoff;
 }
 
-static inline unsigned long vma_last_pgoff(struct vm_area_struct *v)
+static inline pgoff_t vma_last_pgoff(struct vm_area_struct *v)
 {
 	return v->vm_pgoff + ((v->vm_end - v->vm_start) >> PAGE_SHIFT) - 1;
 }
 
 INTERVAL_TREE_DEFINE(struct vm_area_struct, shared.linear.rb,
-		     unsigned long, shared.linear.rb_subtree_last,
+		     pgoff_t, shared.linear.rb_subtree_last,
 		     vma_start_pgoff, vma_last_pgoff,, vma_interval_tree)
 
 /* Insert node immediately after prev in the interval tree */
@@ -32,7 +32,7 @@ void vma_interval_tree_insert_after(struct vm_area_struct *node,
 {
 	struct rb_node **link;
 	struct vm_area_struct *parent;
-	unsigned long last = vma_last_pgoff(node);
+	pgoff_t last = vma_last_pgoff(node);
 
 	VM_BUG_ON(vma_start_pgoff(node) != vma_start_pgoff(prev));
 
@@ -69,7 +69,7 @@ static inline unsigned long avc_last_pgoff(struct anon_vma_chain *avc)
 	return vma_last_pgoff(avc->vma);
 }
 
-INTERVAL_TREE_DEFINE(struct anon_vma_chain, rb, unsigned long, rb_subtree_last,
+INTERVAL_TREE_DEFINE(struct anon_vma_chain, rb, pgoff_t, rb_subtree_last,
 		     avc_start_pgoff, avc_last_pgoff,
 		     static inline, __anon_vma_interval_tree)
 
@@ -91,14 +91,14 @@ void anon_vma_interval_tree_remove(struct anon_vma_chain *node,
 
 struct anon_vma_chain *
 anon_vma_interval_tree_iter_first(struct rb_root *root,
-				  unsigned long first, unsigned long last)
+				  pgoff_t first, pgoff_t last)
 {
 	return __anon_vma_interval_tree_iter_first(root, first, last);
 }
 
 struct anon_vma_chain *
 anon_vma_interval_tree_iter_next(struct anon_vma_chain *node,
-				 unsigned long first, unsigned long last)
+				 pgoff_t first, pgoff_t last)
 {
 	return __anon_vma_interval_tree_iter_next(node, first, last);
 }
diff --git a/mm/ksm.c b/mm/ksm.c
index b6afe0c..a889827 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -1914,7 +1914,7 @@ again:
 
 		anon_vma_lock_read(anon_vma);
 		anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
-					       0, ULONG_MAX) {
+					       0, PGOFF_MAX) {
 			vma = vmac->vma;
 			if (rmap_item->address < vma->vm_start ||
 			    rmap_item->address >= vma->vm_end)
@@ -1967,7 +1967,7 @@ again:
 
 		anon_vma_lock_read(anon_vma);
 		anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
-					       0, ULONG_MAX) {
+					       0, PGOFF_MAX) {
 			vma = vmac->vma;
 			if (rmap_item->address < vma->vm_start ||
 			    rmap_item->address >= vma->vm_end)
@@ -2019,7 +2019,7 @@ again:
 
 		anon_vma_lock_read(anon_vma);
 		anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
-					       0, ULONG_MAX) {
+					       0, PGOFF_MAX) {
 			vma = vmac->vma;
 			if (rmap_item->address < vma->vm_start ||
 			    rmap_item->address >= vma->vm_end)
diff --git a/mm/memory.c b/mm/memory.c
index 4b60011..3e67df3 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -699,8 +699,8 @@ static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
 	if (page)
 		dump_page(page);
 	printk(KERN_ALERT
-		"addr:%p vm_flags:%08lx anon_vma:%p mapping:%p index:%lx\n",
-		(void *)addr, vma->vm_flags, vma->anon_vma, mapping, index);
+		"addr:%p vm_flags:%08lx anon_vma:%p mapping:%p index:%llx\n",
+		(void *)addr, vma->vm_flags, vma->anon_vma, mapping, (unsigned long long)index);
 	/*
 	 * Choose text because data symbols depend on CONFIG_KALLSYMS_ALL=y
 	 */
diff --git a/mm/mmap.c b/mm/mmap.c
index 8d25fdc..0ff3642 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -2008,7 +2008,20 @@ struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)
 			vma_tmp = rb_entry(rb_node,
 					   struct vm_area_struct, vm_rb);
 
-			if (vma_tmp->vm_end > addr) {
+#ifdef CONFIG_ARM_PAGE_SIZE_64KB
+	/* Take into account a wrap-around of the vm_end field to 0x0.
+	** This happends on last arm page with 64KB page sizes.
+	** vm_start =0xFFFF0000, size 64KB.
+	**
+	** This fix is apparently enough, but should be revisited.
+	*/
+				if ((vma_tmp->vm_end - 1) >= addr) {
+					WARN(!(vma_tmp->vm_end),
+						"find vma found the last page,"
+						"ending in address 0x0");
+#else
+				if (vma_tmp->vm_end > addr) {
+#endif
 				vma = vma_tmp;
 				if (vma_tmp->vm_start <= addr)
 					break;
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index aca4364..d7e88b5 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -1747,7 +1747,7 @@ int write_cache_pages(struct address_space *mapping,
 			cycled = 1;
 		else
 			cycled = 0;
-		end = -1;
+		end = PGOFF_MAX;
 	} else {
 		index = wbc->range_start >> PAGE_CACHE_SHIFT;
 		end = wbc->range_end >> PAGE_CACHE_SHIFT;
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 2ee0fd3..6d0ecf2 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -6249,9 +6249,9 @@ static void dump_page_flags(unsigned long flags)
 void dump_page(struct page *page)
 {
 	printk(KERN_ALERT
-	       "page:%p count:%d mapcount:%d mapping:%p index:%#lx\n",
+	       "page:%p count:%d mapcount:%d mapping:%p index:%#llx\n",
 		page, atomic_read(&page->_count), page_mapcount(page),
-		page->mapping, page->index);
+		page->mapping, (unsigned long long)page->index);
 	dump_page_flags(page->flags);
 	mem_cgroup_print_bad_page(page);
 }
diff --git a/mm/percpu.c b/mm/percpu.c
index 8c8e08f..59a6735 100644
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@ -228,6 +228,7 @@ static void pcpu_set_page_chunk(struct page *page, struct pcpu_chunk *pcpu)
 /* obtain pointer to a chunk from a page struct */
 static struct pcpu_chunk *pcpu_get_page_chunk(struct page *page)
 {
+	BUG_ON(page->index > (pgoff_t)(~(unsigned long)0));
 	return (struct pcpu_chunk *)page->index;
 }
 
